
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Machine learning &#8212; Data analysis with Python  documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Machine learning" href="bayes.html" />
    <link rel="prev" title="Image processing" href="image_processing.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="bayes.html" title="Machine learning"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="image_processing.html" title="Image processing"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Data analysis with Python  documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 7ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }
</style>
<div class="section" id="Machine-learning">
<h1>Machine learning<a class="headerlink" href="#Machine-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="Linear-regression">
<h2>Linear regression<a class="headerlink" href="#Linear-regression" title="Permalink to this headline">¶</a></h2>
<p>Regression analysis tries to explain relationships between variables.
One of these variables, called dependend variable, is what we want to
“explain” using one or more <em>explanatory variables</em>. In linear
regression we assume that the dependent variable can be, approximately,
expressed as a linear combination of the explanatory variables. As a
simple example, we might have dependent variable height and an
explanatory variable age. The age of a person can quite well explain the
height of a person, and this relationship is approximately linear for
kids (ages between 1 and 16). Another way of thinking about regression
is fitting a curve to the observed data points. If we have only one
explanatory variable, then this is easy to visualize, as we shall see
below.</p>
<p>We can apply the linear regression easily with scikit-learn package.
Let’s go through some examples.</p>
<p>First make the usual standard imports.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sklearn</span>   <span class="c1"># This imports the scikit-learn library</span>
</pre></div>
</div>
</div>
<p>Then we create some data with approximately the relationship
<span class="math notranslate nohighlight">\(y=2x+1\)</span>, with normally distributed errors.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n</span><span class="o">=</span><span class="mi">20</span>   <span class="c1"># Number of data points</span>
<span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># Standard deviation 1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[ 0.          0.52631579  1.05263158  1.57894737  2.10526316  2.63157895
  3.15789474  3.68421053  4.21052632  4.73684211  5.26315789  5.78947368
  6.31578947  6.84210526  7.36842105  7.89473684  8.42105263  8.94736842
  9.47368421 10.        ]
[ 2.76405235  2.45278879  4.08400114  6.39878794  7.07808431  5.28588001
  8.26587789  8.21706384  9.31783378 10.88428271 11.67035936 14.03322088
 14.39261667 14.80588554 16.18070534 17.12314801 19.33618434 18.68957858
 20.26043612 20.14590426]
</pre></div></div>
</div>
<p>Next we import the LinearRegression class.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>
</pre></div>
</div>
</div>
<p>Now we can fit a line through the data points (x, y):</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">xfit</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">yfit</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span><span class="n">yfit</span><span class="p">)</span>
<span class="c1"># The following will draw as many line segments as there are columns in matrices x and y</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])]),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/linear_regression_9_0.png" src="_images/linear_regression_9_0.png" />
</div>
</div>
<p>The linear regression tries to minimize the sum of squared errors
<span class="math notranslate nohighlight">\(\sum_i (y[i] - \hat{y}[i])^2\)</span>; this is the sum of the lengths of
the red line segments in the above plot. The estimated values
<span class="math notranslate nohighlight">\(\hat{y}[i]\)</span> are denoted by <code class="docutils literal notranslate"><span class="pre">yfit[i]</span></code> in the above code.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Parameters:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficient:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercept:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Parameters: [1.88627741] 2.1379475205341283
Coefficient: 1.8862774144823018
Intercept: 2.1379475205341283
</pre></div></div>
</div>
<p>In this case, the coefficient is the slope of the fitted line, and the
intercept is the point where the fitted line intersects with the y-axis.</p>
<div class="admonition warning">
Note that in scikit-learn the attributes of the model that store the
learned parameters have always an underscore at the end of the name.
This applies to all algorithms in sklearn, not only the linear
regression. This naming style allows one to easily spot the learned
model parameters from other attributes.</div>
<p>The parameters estimated by the regression algorithm were quite close to
the parameters that generated the data: coefficient 2 and intercept 1.
Try experimenting with the number of data points and/or the standard
deviation, to see if you can improve the estimated parameters.</p>
<div class="section" id="Multiple-features">
<h3>Multiple features<a class="headerlink" href="#Multiple-features" title="Permalink to this headline">¶</a></h3>
<p>The previous example had only one explanatory variable. Sometimes this
is called a <em>simple linear regression</em>. The next example illustrates a
more complex regression with multiple explanatory variables.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sample1</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>   <span class="c1"># The three explanatory variables have values 1, 2, and 3, respectively</span>
<span class="n">sample2</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span>   <span class="c1"># Another example of values of explanatory variables</span>
<span class="n">sample3</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>   <span class="c1"># ...</span>
<span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">15</span><span class="p">,</span><span class="mi">39</span><span class="p">,</span><span class="mi">66</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>   <span class="c1"># For values 1,2, and 3 of explanatory variables, the value y=4 was observed, and so on.</span>
</pre></div>
</div>
</div>
<p>Let’s try to fit a linear model to these points:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model2</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">sample1</span><span class="p">,</span><span class="n">sample2</span><span class="p">,</span><span class="n">sample3</span><span class="p">])</span>
<span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">model2</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">model2</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[18]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>(array([2.69086355, 5.05986049, 0.57960308]), 0.0)
</pre></div>
</div>
</div>
<p>Let’s print the various components involved.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">b</span><span class="o">=</span><span class="n">model2</span><span class="o">.</span><span class="n">coef_</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;product:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x:
 [[ 1  2  3]
 [ 4  5  6]
 [ 7  8 10]]
b:
 [[2.29372754]
 [2.46118141]
 [3.12753813]]
y:
 [16.59870476 40.24604601 67.0209254 ]
product:
 [[16.59870476]
 [40.24604601]
 [67.0209254 ]]
</pre></div></div>
</div>
</div>
<div class="section" id="Polynomial-regression">
<h3>Polynomial regression<a class="headerlink" href="#Polynomial-regression" title="Permalink to this headline">¶</a></h3>
<p>It may perhaps come as a surprise that one can fit a polynomial curve to
data points using linear regression. The trick is to add new explanatory
variables to the model. Below we have a single feature x with associated
y values given by third degree polynomial, with some (gaussian) noise
added. It is clearn from the plot that we cannot explain the data well
with a linear function. We add two new features: <span class="math notranslate nohighlight">\(x^2\)</span> and
<span class="math notranslate nohighlight">\(x^3\)</span>. Now the model has three explanatory variables,
<span class="math notranslate nohighlight">\(x, x^2\)</span> and <span class="math notranslate nohighlight">\(x^3\)</span>. The linear regression will find the
coefficients for these variables.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span><span class="mi">150</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="mf">0.15</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">20</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">5</span><span class="o">*</span><span class="n">x</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">5000</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">model_linear</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model_squared</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model_cubic</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x2</span><span class="o">=</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="n">x3</span><span class="o">=</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span>
<span class="n">model_linear</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">model_squared</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">x2</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">model_cubic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">x3</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">xf</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span><span class="mi">150</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">yf_linear</span><span class="o">=</span><span class="n">model_linear</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">yf_squared</span><span class="o">=</span><span class="n">model_squared</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">x2</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">yf_cubic</span><span class="o">=</span><span class="n">model_cubic</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">x3</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xf</span><span class="p">,</span><span class="n">yf_linear</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xf</span><span class="p">,</span><span class="n">yf_squared</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;squared&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xf</span><span class="p">,</span><span class="n">yf_cubic</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;cubic&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:&quot;</span><span class="p">,</span> <span class="n">model_cubic</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercept:&quot;</span><span class="p">,</span> <span class="n">model_cubic</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Coefficients: [-19.23135699 -19.10028669   0.14609221]
Intercept: -1488.9056772937038
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/linear_regression_22_1.png" src="_images/linear_regression_22_1.png" />
</div>
</div>
<p>Linear and squared are not enough to explain the data, but the linear
regression manages to fit quite well a polynomial curve to the data
points, when cubed variables are included!</p>
</div>
<div class="section" id="Ridge-regression">
<h3>Ridge regression<a class="headerlink" href="#Ridge-regression" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="Regression-exercises">
<h2>Regression exercises<a class="headerlink" href="#Regression-exercises" title="Permalink to this headline">¶</a></h2>
<hr class="docutils" />
<div class="admonition note">
Exercise 1 (linear regression)</div>
<p>Write a function <code class="docutils literal notranslate"><span class="pre">fit</span> <span class="pre">line</span></code> that gets one dimensional arrays <code class="docutils literal notranslate"><span class="pre">x</span></code> and
<code class="docutils literal notranslate"><span class="pre">y</span></code> as parameters. The function should return the tuple
<code class="docutils literal notranslate"><span class="pre">(slope,</span> <span class="pre">intercept)</span></code> of the fitted line. Write a main program that
tests the <code class="docutils literal notranslate"><span class="pre">fit_line</span></code> function with some example arrays. The main
function should produce output in the following form:</p>
<p>Slope: 1.0 Intercept: 1.16666666667</p>
<p>Part 2.</p>
<p>Modify your program to plot the fitted line using matplotlib, in
addition to the textual output. Plot also the original data points.</p>
<hr class="docutils" />
<div class="admonition note">
Exercise 2 (mystery data)</div>
<p>Read the tab separated file mystery_data.tsv. Its first five columns
define the features, and the last column is the response. Use
scikit-learns’s LinearRegression to fit this data. Implement function
<code class="docutils literal notranslate"><span class="pre">mystery_data</span></code> that reads this file and learns and returns the
regression coefficients for the five features. You don’t have to fit the
intercept. The main method will print output in the following form:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficient</span> <span class="n">of</span> <span class="n">X1</span> <span class="ow">is</span> <span class="o">...</span>
<span class="n">Coefficient</span> <span class="n">of</span> <span class="n">X2</span> <span class="ow">is</span> <span class="o">...</span>
<span class="n">Coefficient</span> <span class="n">of</span> <span class="n">X3</span> <span class="ow">is</span> <span class="o">...</span>
<span class="n">Coefficient</span> <span class="n">of</span> <span class="n">X4</span> <span class="ow">is</span> <span class="o">...</span>
<span class="n">Coefficient</span> <span class="n">of</span> <span class="n">X5</span> <span class="ow">is</span> <span class="o">...</span>
</pre></div>
</div>
<p>Which features you think are needed to explain the response Y?</p>
<hr class="docutils" />
<div class="admonition note">
Exercise 3 (coefficient of determination)</div>
<p>Using the same data as in the previous exercise, instead of printing the
regression coefficients, print the coefficient of determination. The
coefficient of determination, denoted by R2, tells how well the linear
regression fits the data. The maximum value of the coefficient of
determination is 1. That means the best possible fit.</p>
<p>Part 1.</p>
<p>Using all the features (X1 to X5), fit the data using a linear
regression. Get the coefficient of determination using the <code class="docutils literal notranslate"><span class="pre">score</span></code>
method of the <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> class. Write a function
<code class="docutils literal notranslate"><span class="pre">coefficient_of_determination</span></code> to do all this. It should return a list
containing the R2-score as the only value.</p>
<p>Part 2.</p>
<p>Extend your function so that it also returns R2-scores related to linear
regression with each single feature in turn. The
<code class="docutils literal notranslate"><span class="pre">coefficient_of_determination</span></code>
(<a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">https://en.wikipedia.org/wiki/Coefficient_of_determination</a>) function
should therefore return a list with six R2-scores. To achieve this, your
function should call both the fit method and the score method six times.</p>
<p>The output from the main method should look like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">R2</span><span class="o">-</span><span class="n">score</span> <span class="k">with</span> <span class="n">feature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">X</span><span class="p">:</span> <span class="o">...</span>
<span class="n">R2</span><span class="o">-</span><span class="n">score</span> <span class="k">with</span> <span class="n">feature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">X1</span><span class="p">:</span> <span class="o">...</span>
<span class="n">R2</span><span class="o">-</span><span class="n">score</span> <span class="k">with</span> <span class="n">feature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">X2</span><span class="p">:</span> <span class="o">...</span>
<span class="n">R2</span><span class="o">-</span><span class="n">score</span> <span class="k">with</span> <span class="n">feature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">X3</span><span class="p">:</span> <span class="o">...</span>
<span class="n">R2</span><span class="o">-</span><span class="n">score</span> <span class="k">with</span> <span class="n">feature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">X4</span><span class="p">:</span> <span class="o">...</span>
<span class="n">R2</span><span class="o">-</span><span class="n">score</span> <span class="k">with</span> <span class="n">feature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">X5</span><span class="p">:</span> <span class="o">...</span>
</pre></div>
</div>
<p>How small can the R2-score be? Experiment both with fitting the
intercept and without fitting the intercept.</p>
<hr class="docutils" />
<div class="admonition note">
Exercise 4 ()</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Machine learning</a><ul>
<li><a class="reference internal" href="#Linear-regression">Linear regression</a><ul>
<li><a class="reference internal" href="#Multiple-features">Multiple features</a></li>
<li><a class="reference internal" href="#Polynomial-regression">Polynomial regression</a></li>
<li><a class="reference internal" href="#Ridge-regression">Ridge regression</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Regression-exercises">Regression exercises</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="image_processing.html"
                        title="previous chapter">Image processing</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="bayes.html"
                        title="next chapter">Machine learning</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/linear_regression.ipynb.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="bayes.html" title="Machine learning"
             >next</a> |</li>
        <li class="right" >
          <a href="image_processing.html" title="Image processing"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Data analysis with Python  documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2018, Jarkko Toivonen.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.2.
    </div>
  </body>
</html>